
<!--
help_text: ''
key: summary_spark_plugin_3e6030df-67d4-4978-b3f3-4fed4d127b64
modules:
- flytekitplugins.spark.connector
- flytekitplugins.spark.generic_task
- flytekitplugins.spark.models
- flytekitplugins.spark.task
questions_to_answer: []
type: summary

-->
The Spark Plugin enables the execution of Apache Spark applications, including PySpark, as tasks within workflows. It supports running Spark jobs directly on Kubernetes clusters or by integrating with Databricks for managed Spark environments.

### Core Capabilities

The plugin provides two primary modes for executing Spark tasks:

1.  **Kubernetes-native Spark**: Run PySpark functions as distributed jobs on a Kubernetes cluster, leveraging the Spark on Kubernetes operator.
2.  **Databricks Integration**: Submit and manage Spark jobs directly on Databricks workspaces, allowing seamless orchestration of Databricks notebooks or JARs.

### Configuring Spark Tasks

To define a Spark task, use the `Spark` configuration object. This object allows specifying Spark and Hadoop properties, application details, and Kubernetes resource allocations for the Spark driver and executors.

**`Spark` Configuration**

The `Spark` object configures a Spark job to run on Kubernetes.

*   `spark_conf` (Optional[Dict[str, str]]): A dictionary of Spark configuration properties (e.g., `spark.executor.memory`, `spark.driver.cores`).
*   `hadoop_conf` (Optional[Dict[str, str]]): A dictionary of Hadoop configuration properties.
*   `executor_path` (Optional[str]): The path to the Python binary used for PySpark execution within the Spark containers. Defaults to `/usr/bin/python3` when using the default Spark base image.
*   `applications_path` (Optional[str]): The path to the main application file. For PySpark functions, this typically points to the entrypoint script generated by the platform. Defaults to `local:///usr/local/bin/entrypoint.py` for PySpark tasks.
*   `driver_pod` (Optional[PodTemplate]): A Kubernetes `PodTemplate` to customize the Spark driver pod's resources (e.g., CPU, memory, node selectors).
*   `executor_pod` (Optional[PodTemplate]): A Kubernetes `PodTemplate` to customize the Spark executor pods' resources.

**Example `Spark` Configuration:**

```python
from flytekitplugins.spark import Spark
from flytekit.types.resources import Resource, Resources

spark_config = Spark(
    spark_conf={
        "spark.driver.memory": "2g",
        "spark.executor.memory": "4g",
        "spark.executor.instances": "2",
        "spark.driver.cores": "1",
        "spark.executor.cores": "1",
    },
    driver_pod=PodTemplate(
        primary_container_name="spark-driver",
        resources=Resources(cpu="1", mem="2Gi"),
    ),
    executor_pod=PodTemplate(
        primary_container_name="spark-executor",
        resources=Resources(cpu="1", mem="4Gi"),
    ),
)
```

### Integrating with Databricks

For executing Spark jobs on Databricks, use the `DatabricksV2` configuration object. This extends the `Spark` configuration with Databricks-specific settings.

**`DatabricksV2` Configuration**

*   `databricks_conf` (Optional[Dict[str, Union[str, dict]]]): A dictionary representing the Databricks job configuration, compliant with Databricks Jobs API version 2.1. This allows specifying cluster configurations, libraries, and other job parameters.
*   `databricks_instance` (Optional[str]): The domain name of your Databricks deployment (e.g., `https://<account>.cloud.databricks.com`). This can also be set via the `DEFAULT_DATABRICKS_INSTANCE_ENV_KEY` environment variable in the connector.

**Example `DatabricksV2` Configuration:**

```python
from flytekitplugins.spark import DatabricksV2

databricks_config = DatabricksV2(
    databricks_instance="my-databricks-instance.cloud.databricks.com",
    databricks_conf={
        "new_cluster": {
            "spark_version": "11.3.x-scala2.12",
            "node_type_id": "Standard_DS3_v2",
            "num_workers": 2,
        },
        "libraries": [
            {"pypi": {"package": "pandas"}},
        ],
    },
)
```

**Note on `Databricks`:** The `Databricks` class is deprecated. Always use `DatabricksV2` for new implementations.

### Defining PySpark Tasks

The `PysparkFunctionTask` is the core component that enables Python functions to run as Spark jobs. It automatically manages the Spark session and handles the serialization of your Python code for distributed execution.

When a Python function is decorated with `@task` and provided a `Spark` or `DatabricksV2` configuration, it is transformed into a `PysparkFunctionTask`.

**Automatic SparkSession Management:**
During execution, `PysparkFunctionTask` automatically initializes a `SparkSession` and makes it available within the task's execution context. This `SparkSession` is accessible via `user_params.spark_session` or by directly importing `pyspark.sql.SparkSession` within your task function.

**Example PySpark Task:**

```python
from flytekit import task, workflow
from flytekitplugins.spark import Spark
from pyspark.sql import SparkSession

@task(task_config=Spark(spark_conf={"spark.driver.memory": "2g"}))
def my_spark_task(a: int, b: int) -> float:
    # SparkSession is automatically available
    spark = SparkSession.builder.appName("MySparkApp").getOrCreate()
    df = spark.createDataFrame([(a,), (b,)], ["value"])
    result = df.agg({"value": "sum"}).collect()[0][0]
    return float(result)

@workflow
def my_spark_workflow(x: int = 1, y: int = 2) -> float:
    return my_spark_task(a=x, b=y)
```

**Fast Serialization:**
For remote execution, the plugin supports fast serialization, where the entire working directory (including your task code and its dependencies) is zipped and distributed to Spark executors. This ensures that the Spark cluster has access to all necessary code.

### Execution Flow

1.  **Task Definition**: A Python function is decorated with `@task` and configured with `Spark` or `DatabricksV2`. This creates a `PysparkFunctionTask`.
2.  **Serialization**: During compilation, the `PysparkFunctionTask`'s `get_custom` method serializes the `Spark` or `DatabricksV2` configuration into a `SparkJob` protobuf message. This message is sent to the Flyte backend.
3.  **Backend Handling**:
    *   If the task type is "spark" (configured with `Spark`), the Flyte backend dispatches the job to the Kubernetes Spark operator.
    *   If the task type is "databricks" (configured with `DatabricksV2`), the `DatabricksConnectorV2` handles the job.
4.  **`DatabricksConnectorV2`**: This connector uses the Databricks API to submit, monitor, and manage the lifecycle of the Databricks job. It maps Databricks job states to Flyte task phases and provides a link to the Databricks console for detailed logs.

### Local Development and Testing

Spark tasks can be executed locally for development and testing purposes.

*   **PySpark Tasks**: When running a `PysparkFunctionTask` locally, a local `SparkSession` is initialized, respecting the `spark_conf` settings. This allows for quick iteration without deploying to a cluster.
*   **Databricks Tasks**: While `DatabricksV2` tasks are primarily designed for remote execution, they can be tested locally. However, for local execution of Databricks tasks, the `raw_output_data_prefix` must be set to a remote path (e.g., S3, GCS). If this condition is not met, the task will fall back to local Python execution without interacting with Databricks.

### Generic Spark Applications

For scenarios involving non-Python Spark applications (e.g., Scala, Java, R) or when direct `spark-submit` control is preferred, the `GenericSparkTask` can be used. This task takes a `GenericSparkConf` object, which specifies the `main_class` and `applications_path` for the Spark application. The `execute` method of `GenericSparkTask` directly invokes `spark-submit` locally.

**Example `GenericSparkTask`:**

```python
from flytekit import task, workflow
from flytekitplugins.spark.generic_task import GenericSparkConf, GenericSparkTask

@task(task_config=GenericSparkConf(
    main_class="com.example.MySparkApp",
    applications_path="/path/to/my/spark-app.jar",
    spark_conf={"spark.executor.memory": "1g"}
))
def run_generic_spark_app():
    # This task will execute the spark-submit command
    pass

@workflow
def generic_spark_workflow():
    run_generic_spark_app()
```

### Important Considerations

*   **Container Images**: When using `PysparkFunctionTask`, ensure your container image includes PySpark and any other necessary dependencies. If using `ImageSpec` without a `base_image`, the plugin defaults to a Spark-enabled base image (e.g., `cr.flyte.org/flyteorg/flytekit:spark-<version>`) and sets `executor_path` to `/usr/bin/python3`.
*   **Coexistence of Spark and Databricks**: The `DatabricksConnectorV2` explicitly sets its task type to "databricks". This allows both Kubernetes-native Spark tasks (which use the "spark" task type) and Databricks tasks to be handled by different backend plugins within the same Flyte deployment.
*   **Pod Template Overrides**: If a `PodTemplate` is specified in the `Spark` configuration (for `driver_pod` or `executor_pod`), its `primary_container_name` will override any `primary_container_name` set in the `@task` decorator for the main task container.
<!--
key: summary_spark_plugin_3e6030df-67d4-4978-b3f3-4fed4d127b64
type: summary_end

-->
<!--
code_unit: flytekitplugins.spark.task
code_unit_type: class
help_text: ''
key: example_19c1f8bf-a583-45e8-ad85-65964e6fc6e6
type: example

-->