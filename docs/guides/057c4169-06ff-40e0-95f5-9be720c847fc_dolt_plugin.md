
<!--
help_text: ''
key: summary_dolt_plugin_fba80c66-b5d8-48dd-8f76-f4cb9cfdc8d2
modules:
- flytekitplugins.dolt.schema
questions_to_answer: []
type: summary

-->
The Dolt Plugin integrates Dolt databases with Flyte, enabling tasks to seamlessly interact with version-controlled datasets. It introduces a custom data type, `DoltTable`, which encapsulates both the configuration for accessing a Dolt database and the actual data, typically represented as a Pandas DataFrame. This allows developers to pass Dolt tables directly between Flyte tasks as if they were native Python objects, with the plugin handling the underlying data persistence and retrieval.

### Core Components

The plugin's functionality is built around three primary components: `DoltConfig`, `DoltTable`, and `DoltTableNameTransformer`.

#### DoltConfig

The `DoltConfig` class defines the parameters required for interacting with a Dolt database. It centralizes all necessary connection and operation details.

*   `db_path` (str): Specifies the local path to the Dolt database repository. This is a mandatory field.
*   `tablename` (Optional[str]): The name of the table within the Dolt database to interact with. When saving data, this table is updated. When loading data, the plugin retrieves data from this table.
*   `sql` (Optional[str]): An SQL query used to load data from the Dolt database. This provides an alternative to specifying a `tablename` for data retrieval, allowing for more complex queries.
*   `io_args` (Optional[dict]): A dictionary of additional arguments passed to underlying I/O functions, such as `pandas.read_csv` or `pandas.DataFrame.to_csv`, during data serialization or deserialization.
*   `branch_conf` (Optional[dolt_int.Branch]): Configuration for Dolt branching operations, allowing tasks to interact with specific branches.
*   `meta_conf` (Optional[dolt_int.Meta]): Metadata configuration for Dolt commits, such as author and email.
*   `remote_conf` (Optional[dolt_int.Remote]): Configuration for Dolt remote operations, enabling interaction with remote Dolt repositories.

#### DoltTable

The `DoltTable` class serves as the primary data type for representing Dolt tables within Flyte tasks. It combines the database configuration with the actual data.

*   `config` (DoltConfig): An instance of `DoltConfig` that specifies how to connect to and interact with the Dolt database for this table.
*   `data` (Optional[pandas.DataFrame]): The Pandas DataFrame containing the table's data. When a `DoltTable` is an input to a task, this field is populated by the plugin. When it is an output, the plugin uses this DataFrame to save data to Dolt.

#### DoltTableNameTransformer

The `DoltTableNameTransformer` is a Flyte `TypeTransformer` responsible for converting `DoltTable` objects between their Python representation and Flyte's internal literal types. This transformer is the core mechanism that enables `DoltTable` to function as a first-class type for task inputs and outputs.

When a `DoltTable` is passed as a task output:
1.  The transformer checks if the `DoltTable` instance contains a Pandas DataFrame (`data` is not None) and if a `tablename` is specified in its `config`.
2.  If both conditions are met, the DataFrame is saved to a temporary CSV file.
3.  The data from the temporary CSV is then committed to the specified Dolt database table using an internal Dolt integration utility. A commit message is automatically generated, including the Flyte execution ID for traceability.
4.  Finally, the `DoltTable`'s `config` is serialized into a Flyte `STRUCT` literal, which is then passed to the next task or stored as a workflow output.

When a `DoltTable` is expected as a task input:
1.  The transformer receives a Flyte `STRUCT` literal containing the `DoltConfig`.
2.  It uses the `db_path` and either the `tablename` or `sql` query from the `DoltConfig` to load data from the Dolt database.
3.  The loaded data is temporarily saved to a CSV file and then read into a Pandas DataFrame.
4.  This DataFrame is attached to a new `DoltTable` instance, which is then provided to the task as a Python object.

### Capabilities and Usage

The Dolt Plugin provides several key capabilities for managing data within Flyte workflows:

*   **Automated Data Persistence**: Developers can define `DoltTable` as an input or output type for their tasks. The plugin automatically handles saving Pandas DataFrames to Dolt tables before a task consumes them (if an input) or after a task produces them (if an output).
*   **Configurable Dolt Operations**: Through `DoltConfig`, users can specify the exact Dolt database, table, branch, and commit metadata for each data interaction. This allows for fine-grained control over data versioning and storage.
*   **Version Control for Data**: By leveraging Dolt's underlying capabilities, the plugin ensures that data changes are committed with clear messages, linking them directly to Flyte execution IDs. This provides a robust audit trail and enables reproducible data pipelines.
*   **Seamless Flyte Integration**: `DoltTable` behaves like any other Flyte type, allowing it to be passed between tasks, stored as workflow outputs, and used in conditional logic.

### Practical Implementation

To use the Dolt Plugin, define tasks that accept or return `DoltTable` objects.

```python
import pandas as pd
from flytekit import task, workflow
from flytekitplugins.dolt.schema import DoltConfig, DoltTable

# Assume a Dolt database exists at '/path/to/my/dolt_repo'

@task
def create_initial_dataset(db_path: str, table_name: str) -> DoltTable:
    """
    Creates a new Pandas DataFrame and saves it to a Dolt table.
    """
    data = {"id": [1, 2, 3], "value": ["A", "B", "C"]}
    df = pd.DataFrame(data)

    # Configure how the data should be saved to Dolt
    config = DoltConfig(db_path=db_path, tablename=table_name)

    # Return a DoltTable instance; the transformer will save 'df' to Dolt
    return DoltTable(config=config, data=df)

@task
def transform_dataset(input_table: DoltTable, output_table_name: str) -> DoltTable:
    """
    Loads data from a Dolt table, transforms it, and saves the result
    to a new Dolt table.
    """
    # The 'input_table.data' will be automatically loaded from Dolt
    df = input_table.data
    print(f"Loaded data from Dolt: \n{df}")

    # Perform a transformation
    df["new_value"] = df["value"].apply(lambda x: x + "_transformed")

    # Configure where the transformed data should be saved
    # Reusing the same db_path but a new table name
    output_config = DoltConfig(db_path=input_table.config.db_path, tablename=output_table_name)

    # Return the transformed data as a new DoltTable
    return DoltTable(config=output_config, data=df)

@task
def analyze_dataset(final_table: DoltTable):
    """
    Loads the final dataset from Dolt and performs an analysis.
    """
    df = final_table.data
    print(f"Final data for analysis: \n{df}")
    print(f"Number of rows: {len(df)}")

@workflow
def dolt_data_pipeline(db_repo_path: str, initial_table: str = "raw_data", transformed_table: str = "processed_data"):
    """
    A workflow demonstrating data flow using DoltTable.
    """
    # Task 1: Create and save initial data to Dolt
    created_dolt_table = create_initial_dataset(db_path=db_repo_path, table_name=initial_table)

    # Task 2: Load data from Dolt, transform, and save to a new Dolt table
    transformed_dolt_table = transform_dataset(input_table=created_dolt_table, output_table_name=transformed_table)

    # Task 3: Load the transformed data from Dolt for analysis
    analyze_dataset(final_table=transformed_dolt_table)

```

When executing `dolt_data_pipeline`, ensure that the `db_repo_path` points to an accessible Dolt repository. The plugin handles the data serialization and deserialization, committing changes to the Dolt database at each step where a `DoltTable` is returned as an output.

### Important Considerations

*   **Dolt Database Accessibility**: The Dolt database specified by `db_path` must be accessible from the environment where the Flyte task executes. This typically means the Dolt repository should be mounted or available on the task's execution environment.
*   **Data Flow and Temporary Files**: During serialization and deserialization, data is temporarily written to and read from CSV files. Ensure sufficient disk space and appropriate permissions in the execution environment.
*   **Conditional Saving**: When a `DoltTable` is returned as a task output, the data (`pandas.DataFrame`) is only saved to the Dolt database if both `DoltTable.data` is not `None` AND `DoltConfig.tablename` is not `None`. If `tablename` is omitted, only the `DoltConfig` metadata is passed, and the DataFrame itself is not persisted to Dolt.
*   **Loading Data**: When loading data, `DoltConfig` supports either `tablename` or `sql`. If both are provided, the behavior depends on the internal Dolt integration utility (`dolt_int.load`). It is best practice to specify only one for clarity.
<!--
key: summary_dolt_plugin_fba80c66-b5d8-48dd-8f76-f4cb9cfdc8d2
type: summary_end

-->
<!--
code_unit: flytekitplugins.dolt.schema
code_unit_type: class
help_text: ''
key: example_076b7459-15e7-42e3-8b7b-6bf64871294d
type: example

-->