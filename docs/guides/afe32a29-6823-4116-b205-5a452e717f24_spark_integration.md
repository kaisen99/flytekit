
<!--
help_text: ''
key: summary_spark_integration_8a3314a7-efc5-4915-bad9-eed32b117117
modules:
- flytekitplugins.spark.connector
- flytekitplugins.spark.generic_task
- flytekitplugins.spark.models
- flytekitplugins.spark.pyspark_transformers
- flytekitplugins.spark.schema
- flytekitplugins.spark.sd_transformers
- flytekitplugins.spark.task
questions_to_answer: []
type: summary

-->
Spark Integration

Flyte provides robust integration for running Apache Spark applications, including PySpark, Scala, Java, and R, both on Kubernetes and natively on Databricks. This integration simplifies the orchestration of distributed data processing tasks within your workflows, offering automatic data type handling for common Spark objects like DataFrames and MLlib PipelineModels.

### Running PySpark Tasks on Kubernetes

To execute PySpark tasks directly on your Kubernetes cluster, configure your task with the `Spark` object. This object allows you to specify Spark and Hadoop configurations, as well as customize the Kubernetes pods for the Spark driver and executors.

The `PysparkFunctionTask` plugin handles the transformation of your Python function into a distributable Spark application. It automatically sets up a SparkSession for local execution and prepares the necessary configurations for remote execution on the Flyte backend.

```python
from flytekit import task, workflow
from flytekitplugins.spark import Spark
from pyspark.sql import SparkSession, DataFrame

@task(task_config=Spark(
    spark_conf={
        "spark.driver.memory": "2g",
        "spark.executor.memory": "4g",
        "spark.executor.instances": "2",
        "spark.executor.cores": "1",
    },
    # Optional: Customize driver and executor pods
    # driver_pod=PodTemplate(...),
    # executor_pod=PodTemplate(...),
))
def my_spark_task(input_data: int) -> DataFrame:
    """
    A PySpark task that runs on Kubernetes.
    """
    spark = SparkSession.builder.appName("MySparkApp").getOrCreate()
    data = [(i, f"row_{i}") for i in range(input_data)]
    df = spark.createDataFrame(data, ["id", "value"])
    df.show()
    return df

@workflow
def spark_workflow(num_rows: int = 10):
    df = my_spark_task(input_data=num_rows)
    # Further processing with df
```

The `spark_conf` dictionary allows you to pass standard Spark configuration properties. For more advanced Kubernetes-specific customizations, you can use `driver_pod` and `executor_pod` to provide `PodTemplate` objects, which are converted to `K8sPod` specifications internally by the `PysparkFunctionTask`.

During local execution, the `PysparkFunctionTask` automatically initializes a `SparkSession` and propagates the `PYTHONPATH` to ensure a consistent environment. For remote execution, the `SparkJob` model captures these configurations, including the `executor_path` (Python binary) and `application_file` (entrypoint script), for the Flyte backend.

### Running PySpark Tasks on Databricks

For executing PySpark tasks natively on Databricks, use the `DatabricksV2` configuration object. This leverages the `DatabricksConnector` to interact directly with the Databricks Jobs API, submitting your Spark application as a Databricks job run.

```python
from flytekit import task, workflow
from flytekitplugins.spark import DatabricksV2
from pyspark.sql import SparkSession, DataFrame

@task(task_config=DatabricksV2(
    databricks_instance="<your-databricks-instance>.cloud.databricks.com",
    databricks_conf={
        "run_name": "my-flyte-databricks-job",
        "new_cluster": {
            "spark_version": "12.2.x-scala2.12",
            "node_type_id": "Standard_DS3_v2",
            "num_workers": 2,
        },
        "spark_python_task": {
            "python_file": "dbfs:/path/to/your/script.py", # Or use local:/// for Flyte-managed code
            "parameters": ["--input", "100"],
        },
    },
))
def my_databricks_task(input_data: int) -> DataFrame:
    """
    A PySpark task that runs on Databricks.
    """
    spark = SparkSession.builder.appName("MyDatabricksApp").getOrCreate()
    data = [(i, f"db_row_{i}") for i in range(input_data)]
    df = spark.createDataFrame(data, ["id", "value"])
    df.show()
    return df

@workflow
def databricks_workflow(num_rows: int = 10):
    df = my_databricks_task(input_data=num_rows)
    # Further processing with df
```

The `databricks_instance` specifies your Databricks workspace URL. The `databricks_conf` dictionary should conform to the Databricks Jobs API 2.1 request structure, allowing you to define cluster configurations, libraries, and task parameters.

The `DatabricksConnector` uses `DatabricksJobMetadata` to track the `run_id` of the submitted job and polls the Databricks API for status updates. For local execution of Databricks tasks, ensure your `raw_output_data_prefix` is configured to a remote storage location (e.g., S3, GCS), as the connector requires remote access for job submission and status retrieval.

**Note:** The `Databricks` class is deprecated. Always use `DatabricksV2` for Databricks task configurations. The `DatabricksConnectorV2` is internally used to ensure that tasks configured with `DatabricksV2` are handled by a distinct "databricks" task type, allowing it to coexist with the generic "spark" task type in the same Flyte deployment.

### Handling PySpark Data Types

Flyte provides built-in transformers for common PySpark data types, enabling seamless passing of these objects as inputs and outputs between tasks.

#### PySpark DataFrames (`pyspark.sql.DataFrame`)

PySpark DataFrames are automatically serialized to and deserialized from Flyte's Schema or StructuredDataset types. The default storage format used is Parquet.

The `SparkDataFrameTransformer` handles the conversion of `pyspark.sql.DataFrame` to and from Flyte's `Literal` representation. When writing, `SparkDataFrameSchemaWriter` saves the DataFrame to a remote path in Parquet format. When reading, `SparkDataFrameSchemaReader` loads the DataFrame from the specified URI.

For `StructuredDataset` types, `SparkToParquetEncodingHandler` and `ParquetToSparkDecodingHandler` manage the serialization and deserialization, respectively, ensuring that DataFrames are stored and retrieved efficiently in Parquet format.

```python
from flytekit import task
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

@task
def create_dataframe() -> DataFrame:
    spark = SparkSession.builder.appName("DataFrameCreator").getOrCreate()
    schema = StructType([
        StructField("name", StringType(), True),
        StructField("age", IntegerType(), True)
    ])
    data = [("Alice", 1), ("Bob", 2)]
    df = spark.createDataFrame(data, schema)
    return df

@task
def process_dataframe(df: DataFrame) -> DataFrame:
    df.show()
    return df.filter(df.age > 1)

@workflow
def dataframe_workflow():
    df1 = create_dataframe()
    df2 = process_dataframe(df1)
```

#### PySpark MLlib PipelineModels (`pyspark.ml.PipelineModel`)

PySpark MLlib `PipelineModel` objects are also automatically handled, allowing you to pass trained models between tasks. They are serialized into a binary format.

The `PySparkPipelineModelTransformer` manages the serialization and deserialization of `PipelineModel` objects. When a `PipelineModel` is returned from a task, it is written to a remote directory. When consumed by another task, it is loaded from that directory.

```python
from flytekit import task
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.ml.classification import LogisticRegression

@task
def train_model() -> PipelineModel:
    spark = SparkSession.builder.appName("ModelTrainer").getOrCreate()
    training_data = spark.createDataFrame([
        (0, "a b c d e spark", 1.0),
        (1, "b d", 0.0),
        (2, "spark f g h", 1.0),
        (3, "g h eye j k spark", 1.0),
        (4, "a b c d e spark", 1.0),
        (5, "b d", 0.0),
        (6, "spark f g h", 1.0),
        (7, "g h eye j k spark", 1.0)
    ], ["id", "text", "label"])

    tokenizer = Tokenizer(inputCol="text", outputCol="words")
    hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="rawFeatures", numFeatures=20)
    idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol="features")
    lr = LogisticRegression(maxIter=10, regParam=0.001)
    pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])

    model = pipeline.fit(training_data)
    return model

@task
def predict_with_model(model: PipelineModel) -> None:
    spark = SparkSession.builder.appName("ModelPredictor").getOrCreate()
    test_data = spark.createDataFrame([
        (8, "spark i j k"),
        (9, "l m n"),
        (10, "spark a b c")
    ], ["id", "text"])

    predictions = model.transform(test_data)
    predictions.select("id", "text", "probability", "prediction").show()

@workflow
def model_workflow():
    trained_model = train_model()
    predict_with_model(model=trained_model)
```

### Running Generic Spark Applications

For Spark applications written in Scala, Java, or R, or for pre-compiled JARs, use the `GenericSparkTask` with a `GenericSparkConf` object. This allows you to specify the main class and the application file.

```python
from flytekit import task, workflow
from flytekitplugins.spark import GenericSparkConf, GenericSparkTask

@task(task_config=GenericSparkConf(
    main_class="com.example.MySparkApp",
    applications_path="local:///path/to/your/spark-app.jar", # Or a remote path like s3://...
    spark_conf={
        "spark.driver.memory": "1g",
        "spark.executor.memory": "2g",
    }
))
def run_generic_spark_app(input_arg: str) -> str:
    """
    A generic Spark task for Java/Scala/R applications.
    The actual execution is handled by the Flyte backend.
    """
    # This function body is a placeholder; the actual Spark job runs externally.
    print(f"Running generic Spark app with input: {input_arg}")
    return "Generic Spark app completed"

@workflow
def generic_spark_workflow():
    result = run_generic_spark_app(input_arg="hello_spark")
```

The `GenericSparkTask` constructs a `SparkJob` with the specified `main_class` and `application_file`, which the Flyte backend then uses to launch the Spark job.

### Advanced Configuration

The `SparkJob` object serves as the internal representation of a Spark task's configuration that is sent to the Flyte backend. When you define a task with `Spark` or `DatabricksV2`, the `PysparkFunctionTask.get_custom` method converts your Python-side configuration into this `SparkJob` protobuf message. This includes all Spark, Hadoop, Databricks, and Kubernetes pod specifications.

You can customize the Kubernetes pod specifications for the Spark driver and executors using the `driver_pod` and `executor_pod` attributes of the `Spark` configuration object. These accept `PodTemplate` objects, which are then converted to `K8sPod` objects internally by the `to_k8s_pod` helper method.

### Important Considerations

*   **Databricks Deprecation:** Always use `DatabricksV2` instead of the deprecated `Databricks` class for Databricks task configurations.
*   **Local Databricks Execution:** When running Databricks tasks locally, ensure your Flyte execution environment's `raw_output_data_prefix` is configured to a remote storage location (e.g., S3, GCS). This is necessary for the Databricks connector to interact with the remote Databricks API and store intermediate data.
*   **DataFrame Format:** Currently, PySpark DataFrames are primarily supported for serialization and deserialization using the Parquet format.
*   **Task Type Coexistence:** The `DatabricksConnectorV2` is specifically designed to use a distinct "databricks" task type, allowing you to run both generic "spark" tasks (on Kubernetes) and "databricks" tasks (on Databricks) within the same Flyte deployment without conflicts.
<!--
key: summary_spark_integration_8a3314a7-efc5-4915-bad9-eed32b117117
type: summary_end

-->
<!--
code_unit: flytekitplugins.spark.examples.pyspark_task
code_unit_type: class
help_text: ''
key: example_8edc9f4f-9da0-4e93-9fa6-305f8f3e67fc
type: example

-->
<!--
code_unit: flytekitplugins.spark.examples.databricks_task
code_unit_type: class
help_text: ''
key: example_1afc884c-075a-4e40-beca-7ca94a7192e6
type: example

-->