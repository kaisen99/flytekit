
<!--
help_text: ''
key: summary_openai_batch_api_d4381f86-fc66-4bc3-852c-33cb3022ae23
modules:
- flytekitplugins.openai.batch.connector
- flytekitplugins.openai.batch.task
questions_to_answer: []
type: summary

-->
The OpenAI Batch API integration provides a robust mechanism for asynchronously processing large volumes of requests with OpenAI models directly within Flyte workflows. This is particularly useful for tasks like large-scale data labeling, content generation, or fine-tuning data preparation, where immediate, synchronous responses are not required, and cost-efficiency is paramount.

This integration leverages Flyte's asynchronous task capabilities to manage the lifecycle of OpenAI batch jobs, from input file upload to result retrieval.

### Core Components

The integration is built around several key components that manage file operations, batch job submission, and status monitoring:

*   **File Management Tasks:**
    *   `UploadJSONLFileTask`: Facilitates uploading input data in JSONL format to OpenAI's file storage. This task takes a `JSONLFile` as input and returns the OpenAI `file_id` as a string, which is then used to initiate a batch job.
    *   `DownloadJSONFilesTask`: Handles the retrieval of output and error files generated by completed OpenAI batch jobs. It takes the raw result dictionary from a completed batch job and provides a `BatchResult` object containing local `JSONLFile` references for the output and error files.
    *   `OpenAIFileConfig`: A configuration object used by file management tasks to specify the OpenAI organization and the `Secret` containing the API key.

*   **Batch Job Execution Task:**
    *   `BatchEndpointTask`: The primary task for submitting and managing OpenAI batch jobs. This task initiates a batch job using an uploaded `input_file_id` and a configuration dictionary. It monitors the job's status and returns a dictionary containing the final batch job details, including `output_file_id` and `error_file_id` upon completion.

*   **OpenAI Connector:**
    *   `BatchEndpointConnector`: An internal asynchronous connector that directly interfaces with the OpenAI Batch API. It handles the `create`, `get`, and `delete` operations for batch jobs. This connector translates OpenAI batch statuses into Flyte's execution phases using the `State` enumeration.

*   **Status Mapping:**
    *   `State`: An enumeration that maps OpenAI batch statuses (e.g., "in_progress", "completed", "failed") to Flyte's `Running`, `Success`, and `Failed` phases, ensuring consistent workflow state reporting.

*   **Result Structure:**
    *   `BatchResult`: A data class representing the output of the `DownloadJSONFilesTask`. It contains optional `output_file` and `error_file` attributes, both of type `JSONLFile`, pointing to the locally downloaded batch results.

### Usage Workflow

Implementing an OpenAI batch processing workflow typically involves three main steps: uploading the input file, submitting the batch job, and downloading the results.

#### 1. Uploading Input Data

Before submitting a batch job, the input data must be uploaded to OpenAI's file storage. The input data must be in JSONL format, where each line is a JSON object representing a single request (e.g., a chat completion request).

Use the `UploadJSONLFileTask` to perform this operation. Configure it with your OpenAI organization ID and a `Secret` for your API key.

```python
import flytekit
from flytekit import task, workflow
from flytekit.types.file import JSONLFile
from flytekitplugins.openai.batch.task import OpenAIFileConfig, UploadJSONLFileTask, BatchEndpointTask, DownloadJSONFilesTask

# Define your OpenAI API key secret
# Ensure this secret is configured in your Flyte environment
OPENAI_API_KEY_SECRET = flytekit.Secret(group="openai", key="api_key")

# Configure the OpenAI organization (optional, if not set globally)
OPENAI_ORG_ID = "org-your_openai_organization_id" # Replace with your actual organization ID

# Initialize the upload task
upload_jsonl_task = UploadJSONLFileTask(
    name="upload_openai_batch_input",
    task_config=OpenAIFileConfig(
        secret=OPENAI_API_KEY_SECRET,
        openai_organization=OPENAI_ORG_ID,
    ),
)

@task
def create_sample_jsonl_file() -> JSONLFile:
    # In a real scenario, this would come from your data source
    # This example creates a simple JSONL file for demonstration
    content = """
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": "What is the capital of France?"}]}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": "Who wrote 'Romeo and Juliet'?"}]}}
"""
    file_path = "input.jsonl"
    with open(file_path, "w") as f:
        f.write(content.strip())
    return JSONLFile(file_path)

@workflow
def upload_workflow() -> str:
    jsonl_file = create_sample_jsonl_file()
    file_id = upload_jsonl_task(jsonl_in=jsonl_file)
    return file_id
```

The `upload_workflow` will execute `create_sample_jsonl_file` to generate a local JSONL file, then use `upload_jsonl_task` to upload it to OpenAI, returning the `file_id`.

#### 2. Submitting a Batch Job

Once the input file is uploaded and you have its `file_id`, you can submit a batch job using the `BatchEndpointTask`. This task requires the `input_file_id` and a `config` dictionary that mirrors the parameters for the OpenAI `batches.create` API call.

Common configuration parameters include:
*   `completion_window`: The time window for the batch to complete (e.g., "24h").
*   `endpoint`: The API endpoint for the batch job (e.g., "/v1/chat/completions").
*   `metadata`: Optional metadata for the batch job.

```python
# Initialize the batch endpoint task
batch_endpoint_task = BatchEndpointTask(
    name="run_openai_batch_job",
    openai_organization=OPENAI_ORG_ID,
    config={
        "completion_window": "24h",
        "endpoint": "/v1/chat/completions",
        "metadata": {"workflow_name": "my_batch_workflow"},
    },
)

@workflow
def batch_processing_workflow(input_file_id: str) -> dict:
    batch_result = batch_endpoint_task(input_file_id=input_file_id)
    return batch_result

# Example of how to chain them
@workflow
def full_openai_batch_workflow() -> dict:
    jsonl_file = create_sample_jsonl_file()
    file_id = upload_jsonl_task(jsonl_in=jsonl_file)
    batch_job_details = batch_endpoint_task(input_file_id=file_id)
    return batch_job_details
```

The `batch_endpoint_task` will asynchronously manage the OpenAI batch job. Its output `batch_job_details` is a dictionary containing the full response from the OpenAI `batches.retrieve` API call, including the `status`, `output_file_id`, and `error_file_id`.

#### 3. Downloading Output and Error Files

After the `BatchEndpointTask` completes successfully, the `batch_job_details` output will contain `output_file_id` and potentially `error_file_id`. Use these IDs with the `DownloadJSONFilesTask` to retrieve the actual result files.

```python
# Initialize the download task
download_json_files_task = DownloadJSONFilesTask(
    name="download_openai_batch_results",
    task_config=OpenAIFileConfig(
        secret=OPENAI_API_KEY_SECRET,
        openai_organization=OPENAI_ORG_ID,
    ),
)

@workflow
def complete_batch_workflow() -> BatchResult:
    jsonl_file = create_sample_jsonl_file()
    file_id = upload_jsonl_task(jsonl_in=jsonl_file)
    batch_job_details = batch_endpoint_task(input_file_id=file_id)
    downloaded_files = download_json_files_task(batch_endpoint_result=batch_job_details)
    return downloaded_files

# To access the downloaded files:
# result = complete_batch_workflow()
# if result.output_file:
#     print(f"Output file path: {result.output_file.path}")
#     with open(result.output_file.path, "r") as f:
#         for line in f:
#             print(line.strip())
# if result.error_file:
#     print(f"Error file path: {result.error_file.path}")
```

The `downloaded_files` output will be a `BatchResult` object. You can access the downloaded output and error files via `downloaded_files.output_file` and `downloaded_files.error_file`, respectively. These are `JSONLFile` objects, which provide a path to the locally downloaded file.

### Configuration and Secrets Management

All tasks interacting with the OpenAI API require an API key and optionally an organization ID.

*   **API Key:** The API key must be stored as a Flyte `Secret`. The `OpenAIFileConfig` class, used by `UploadJSONLFileTask` and `DownloadJSONFilesTask`, takes a `flytekit.Secret` object. The `BatchEndpointConnector` (used by `BatchEndpointTask`) retrieves the API key using `get_connector_secret(secret_key=OPENAI_API_KEY)`. Ensure your `OPENAI_API_KEY` environment variable or a configured secret group/key matches this.
*   **Organization ID:** The `openai_organization` parameter can be provided to `OpenAIFileConfig` and `BatchEndpointTask`. This is useful if you manage multiple OpenAI organizations.

### Error Handling and Statuses

The `BatchEndpointConnector` maps OpenAI batch statuses to Flyte phases:
*   `in_progress`, `finalizing`, `validating` map to `Running`.
*   `completed` maps to `Success`.
*   `failed`, `cancelled`, `cancelling`, `expired` map to `Failed`.

If a batch job fails, the `BatchEndpointTask` will report a `Failed` phase, and the `message` attribute of the `Resource` object (internally managed by the connector) may contain a detailed error message from OpenAI. The `batch_job_details` dictionary returned by `BatchEndpointTask` will also contain an `errors` field if the batch failed, which can be inspected for more granular error information.

### Best Practices and Considerations

*   **Input File Format:** Always ensure your input files are valid JSONL, with each line representing a single API request object.
*   **Asynchronous Nature:** OpenAI batch jobs are inherently asynchronous. The `BatchEndpointTask` will wait for the job to complete, but the actual processing happens on OpenAI's side.
*   **Resource Allocation:** The file upload and download tasks (`UploadJSONLFileTask`, `DownloadJSONFilesTask`) are configured with a default memory request of `700Mi`. Adjust this based on the size of the files being processed.
*   **Container Images:** The plugin uses specific container images (`OpenAIFileDefaultImages`) that include the necessary OpenAI SDK. Ensure your Flyte environment can access these images or provide custom ones if needed.
*   **Cost Management:** Batch API pricing differs from synchronous API calls. Monitor your OpenAI usage and costs, especially for large-scale operations.
*   **Idempotency:** OpenAI batch jobs are not inherently idempotent. If a workflow retries a `BatchEndpointTask`, it will submit a new batch job. Design your workflows to handle this if necessary, perhaps by checking for existing batch jobs or using unique `custom_id` values in your input requests.
<!--
key: summary_openai_batch_api_d4381f86-fc66-4bc3-852c-33cb3022ae23
type: summary_end

-->
<!--
code_unit: flytekitplugins.openai.batch.examples.openai_batch_inference
code_unit_type: class
help_text: ''
key: example_ed732972-42bb-4ba7-9875-9b46da6e3f05
type: example

-->