
<!--
help_text: ''
key: summary_bigquery_plugin_6cf3a247-5c41-4064-8433-6f99dc0a0510
modules:
- flytekitplugins.bigquery.connector
- flytekitplugins.bigquery.task
questions_to_answer: []
type: summary

-->
The BigQuery Plugin enables the execution of BigQuery SQL queries as native Flyte tasks. This integration allows developers to define, manage, and monitor BigQuery jobs directly within their Flyte workflows, leveraging Flyte's scheduling, data passing, and observability features.

### Defining BigQuery Tasks

To define a BigQuery task, use the `BigQueryTask` class. This class extends Flyte's `SQLTask` and `AsyncConnectorExecutorMixin`, providing capabilities for defining SQL queries and managing their asynchronous execution.

A `BigQueryTask` requires the following:

*   **`name`**: A unique name for the task within the project.
*   **`query_template`**: The SQL query to be executed. This template supports Flyte's Golang templating syntax, allowing for dynamic insertion of input parameters.
*   **`task_config`**: An instance of `BigQueryConfig` that specifies the BigQuery project, location, and optional job configuration.
*   **`inputs`**: An optional dictionary mapping input names to their Python types. These inputs are automatically converted to BigQuery query parameters.
*   **`output_structured_dataset_type`**: An optional type hint for `StructuredDataset` if the query produces a result set that should be captured as a Flyte output.

**Example:**

```python
from flytekitplugins.bigquery import BigQueryConfig, BigQueryTask
from flytekit.types.structured.structured_dataset import StructuredDataset
from typing import Type

# Define the schema for the output StructuredDataset
# This should match the schema of the BigQuery table produced by the query
class MyBigQueryOutput(StructuredDataset):
    id: int
    name: str

# Configure the BigQuery task
bigquery_config = BigQueryConfig(
    ProjectID="your-gcp-project-id",
    Location="US", # Optional, defaults to BigQuery's default location if not specified
)

# Define a BigQuery task that takes an input and produces an output
bigquery_query_task = BigQueryTask(
    name="my_bigquery_query",
    query_template="SELECT id, name FROM `{{.project_id}}.my_dataset.my_table` WHERE id = {{.input_id}}",
    task_config=bigquery_config,
    inputs={"input_id": int},
    output_structured_dataset_type=Type[MyBigQueryOutput],
)

# Example of a task that does not produce an output (e.g., DDL or DML without results)
bigquery_dml_task = BigQueryTask(
    name="my_bigquery_dml",
    query_template="INSERT INTO `{{.project_id}}.my_dataset.another_table` (value) VALUES ('{{.some_value}}')",
    task_config=bigquery_config,
    inputs={"some_value": str},
)
```

### Configuring BigQuery Tasks

The `BigQueryConfig` class provides the necessary configuration for a BigQuery task:

*   **`ProjectID`** (str): The Google Cloud Project ID where the BigQuery job will be executed. This is a mandatory field.
*   **`Location`** (Optional[str]): The geographic location where the BigQuery job will run (e.g., "US", "EU", "us-central1"). If not specified, BigQuery's default location for the project is used.
*   **`QueryJobConfig`** (Optional[`google.cloud.bigquery.QueryJobConfig`]): An optional object from the `google-cloud-bigquery` library to provide advanced configuration for the BigQuery query job. This allows setting properties like destination table, write disposition, default dataset, and more.

**Example with `QueryJobConfig`:**

```python
from google.cloud import bigquery
from flytekitplugins.bigquery import BigQueryConfig, BigQueryTask
from flytekit.types.structured.structured_dataset import StructuredDataset
from typing import Type

class AggregatedData(StructuredDataset):
    category: str
    total_count: int

# Define a QueryJobConfig to specify a destination table and write disposition
job_config = bigquery.QueryJobConfig(
    destination="your-gcp-project-id.my_dataset.aggregated_results",
    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE, # Overwrite table if it exists
)

bigquery_config_with_job_config = BigQueryConfig(
    ProjectID="your-gcp-project-id",
    Location="US",
    QueryJobConfig=job_config,
)

# Task that writes results to a specific destination table
aggregate_task = BigQueryTask(
    name="aggregate_data_to_table",
    query_template="SELECT category, COUNT(*) as total_count FROM `{{.project_id}}.my_dataset.source_table` GROUP BY category",
    task_config=bigquery_config_with_job_config,
    output_structured_dataset_type=Type[AggregatedData], # Output type should match the destination table schema
)
```

### Inputs and Outputs

When defining `inputs` for a `BigQueryTask`, Flytekit automatically converts the provided Python values to `bigquery.ScalarQueryParameter` objects, which are then passed to the BigQuery query. The `query_template` uses Golang templating syntax (e.g., `{{.input_name}}`) to reference these parameters.

If a query produces a result set, specify the `output_structured_dataset_type` parameter with a `StructuredDataset` type. The BigQuery Plugin will then capture the destination table of the query (if one is implicitly or explicitly created) and expose it as a `StructuredDataset` output named `results`. This `StructuredDataset` will have a URI pointing to the BigQuery table (e.g., `bq://project:dataset.table`).

### Execution and Monitoring

The `BigQueryConnector` is responsible for the actual interaction with the BigQuery API. When a `BigQueryTask` is executed:

1.  The `BigQueryConnector` receives the task definition and inputs.
2.  It initializes a BigQuery client using the specified `ProjectID` and `Location`.
3.  It constructs a `QueryJobConfig` from the `task_config` and the provided inputs, converting Python types to appropriate BigQuery scalar query parameters.
4.  It submits the SQL query as an asynchronous BigQuery job using `client.query()`.
5.  The connector continuously polls the BigQuery job status.
6.  Job states (e.g., `RUNNING`, `DONE`, `FAILED`) are mapped to Flyte task execution phases (`RUNNING`, `SUCCEEDED`, `FAILED`).
7.  For monitoring, a direct link to the BigQuery console for the specific job is provided in the Flyte UI's task logs. This link allows detailed inspection of job progress, errors, and results within the BigQuery console.
8.  If the BigQuery job fails, any errors reported by BigQuery are captured and displayed in the Flyte task logs.
9.  Upon successful completion, if an `output_structured_dataset_type` was specified, the connector identifies the destination table of the query and provides its URI as the `results` output.
10. If a task is canceled, the `BigQueryConnector` attempts to cancel the corresponding BigQuery job using `client.cancel_job()`.

### Best Practices and Considerations

*   **Permissions:** Ensure the service account or user executing the Flyte workflow has the necessary Google Cloud IAM permissions for BigQuery, including `bigquery.jobs.create`, `bigquery.jobs.get`, `bigquery.jobs.update`, and `bigquery.jobs.cancel`, as well as permissions to read/write data to the relevant datasets and tables.
*   **Query Templating:** Familiarize yourself with Flyte's Golang templating syntax for `query_template`. This is crucial for passing dynamic inputs to your BigQuery queries.
*   **Cost Management:** BigQuery queries can incur significant costs, especially for large datasets. Design your queries efficiently and consider using `QueryJobConfig` to manage resource usage (e.g., setting `maximum_bytes_billed`).
*   **Idempotency:** When using `QueryJobConfig` to write to destination tables, carefully consider the `write_disposition` (e.g., `WRITE_TRUNCATE` to overwrite, `WRITE_APPEND` to append, `WRITE_EMPTY` to fail if the table exists). This impacts the idempotency of your tasks.
*   **Error Handling:** While the plugin reports BigQuery job errors, robust error handling within your SQL queries (e.g., using `SAFE_CAST`, `IFNULL`) can prevent job failures for data-related issues.
*   **StructuredDataset Schema:** When defining `output_structured_dataset_type`, ensure the schema of your `StructuredDataset` accurately reflects the schema of the BigQuery table produced by your query. Mismatches can lead to issues when downstream tasks attempt to consume the data.
<!--
key: summary_bigquery_plugin_6cf3a247-5c41-4064-8433-6f99dc0a0510
type: summary_end

-->
<!--
code_unit: flytekitplugins.bigquery.task
code_unit_type: class
help_text: ''
key: example_0270ed68-3370-4438-bcb8-c7a4dc5b25ab
type: example

-->