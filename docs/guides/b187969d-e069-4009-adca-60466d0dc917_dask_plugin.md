
<!--
help_text: ''
key: summary_dask_plugin_f2ac0917-cec9-42f6-93b0-f591d58db398
modules:
- flytekitplugins.dask.models
- flytekitplugins.dask.task
questions_to_answer: []
type: summary

-->
The Dask plugin enables running Dask computations as tasks within Flyte workflows. It provisions a dedicated Dask cluster, consisting of a scheduler and one or more worker pods, for each task execution. This allows distributed data processing and machine learning workloads to leverage Flyte's orchestration capabilities.

### Defining Dask Tasks

Define a Dask task by decorating a Python function with `DaskTask`. The `DaskTask` constructor accepts a `task_config` parameter, which must be an instance of the `Dask` configuration object. This `Dask` object specifies the setup for the Dask cluster.

### Dask Cluster Configuration

The `Dask` configuration object allows detailed specification of the Dask cluster's components: the scheduler and the worker group.

#### Scheduler Configuration

The `Scheduler` object defines the properties for the Dask scheduler pod.

*   **`image`**: An optional custom Docker image for the scheduler pod. If not specified, the task's default image is used. The image must have `dask[distributed]` installed and should maintain a consistent Python environment with the task runner and worker pods to avoid compatibility issues.
*   **`requests`**: Optional resource requests (e.g., CPU, memory) for the scheduler pod. These define the minimum resources guaranteed to the scheduler.
*   **`limits`**: Optional resource limits for the scheduler pod. These define the maximum resources the scheduler pod can consume.

#### Worker Group Configuration

The `WorkerGroup` object defines the properties for the default group of Dask worker pods.

*   **`number_of_workers`**: The number of worker pods in the group. This defaults to `1` if not specified. Each worker group requires at least one worker.
*   **`image`**: An optional custom Docker image for the worker pods. Similar to the scheduler image, it must have `dask[distributed]` installed and a consistent Python environment with the scheduler and task runner.
*   **`requests`**: Optional resource requests for each worker pod.
*   **`limits`**: Optional resource limits for each worker pod.

### Implementing a Dask Task

The decorated Python function executes within the context of the provisioned Dask cluster. Inside the function, connect to the Dask scheduler using `dask.distributed.Client()`. The client automatically discovers and connects to the cluster provisioned by Flyte.

```python
from flytekit import task
from flytekitplugins.dask import DaskTask, Dask, Scheduler, WorkerGroup
from flytekit.core.resources import Resources
from dask.distributed import Client
import dask.array as da

@DaskTask(
    task_config=Dask(
        scheduler=Scheduler(
            image="ghcr.io/flyteorg/flytekit:py3.9-dask-latest", # Example custom image
            requests=Resources(cpu="1", mem="2Gi"),
            limits=Resources(cpu="2", mem="4Gi"),
        ),
        workers=WorkerGroup(
            number_of_workers=2,
            image="ghcr.io/flyteorg/flytekit:py3.9-dask-latest", # Example custom image
            requests=Resources(cpu="2", mem="4Gi"),
            limits=Resources(cpu="4", mem="8Gi"),
        ),
    )
)
def my_dask_array_task(n: int) -> float:
    """
    A Dask task that performs a distributed array computation.
    """
    with Client() as client:
        print(f"Dask client connected to: {client.scheduler_info['address']}")
        # Perform Dask computations
        x = da.random.random((n, n), chunks=(1000, 1000))
        y = x + x.T
        z = y[::2, n // 2:].mean(axis=1)
        result = z.compute()
        return float(result.sum())

# To run this task in a workflow:
# from flytekit import workflow
# @workflow
# def dask_workflow(size: int) -> float:
#     return my_dask_array_task(n=size)
```

### Resource Management

The `requests` and `limits` specified in the `Scheduler` and `WorkerGroup` configurations are directly translated to resource requirements for the underlying Kubernetes pods. If `requests` or `limits` are not explicitly defined for the scheduler or workers, the resource specifications provided for the overall Flyte task (e.g., via `@task(requests=...)`) will be applied to the respective pods. This provides flexibility to either define resources at the cluster component level or inherit them from the main task definition.

### Image Management and Environment Consistency

When specifying custom images for the scheduler or workers, ensure these images have `dask[distributed]` installed. Crucially, the Python environment (including Python version and installed libraries) across the task runner pod, the scheduler pod, and all worker pods should be consistent. Inconsistencies can lead to serialization errors or unexpected behavior during Dask computations. If no custom image is specified for a component, it defaults to using the same image the Flyte task was registered with.

### Considerations and Best Practices

*   **Single Worker Group:** The current Dask plugin implementation supports a single default worker group per Dask cluster.
*   **Minimum Workers:** The `number_of_workers` for a `WorkerGroup` must be at least `1`.
*   **Client Connection:** Always initialize `dask.distributed.Client()` within the Dask task function. This ensures the client connects to the D Dask cluster provisioned for that specific task execution.
*   **Dependency Management:** Ensure all necessary Python libraries for your Dask computation are installed in the Docker images used by the scheduler and worker pods.
<!--
key: summary_dask_plugin_f2ac0917-cec9-42f6-93b0-f591d58db398
type: summary_end

-->
<!--
code_unit: flytekitplugins.dask.task
code_unit_type: class
help_text: ''
key: example_06177017-5484-4995-83a8-870ecd82e11f
type: example

-->